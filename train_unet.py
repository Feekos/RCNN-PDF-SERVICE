import os
import json
import cv2
import subprocess
import numpy as np
import torch
import time
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from PIL import Image
from unet_model import UNet
from tqdm import tqdm
import matplotlib.pyplot as plt

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π ---
ANNOTATION_DIR = "annotations"

os.makedirs(ANNOTATION_DIR, exist_ok=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω .json —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ annotations
annotation_files = [f for f in os.listdir(ANNOTATION_DIR) if f.endswith('.json')]
if not annotation_files:
    print("‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –ø—É—Å—Ç–∞. –ó–∞–ø—É—Å–∫ generate_annotations.py...")
    try:
        result = subprocess.run(
            ["python", "generate_annotations.py"],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        print("‚úÖ –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
        print(result.stdout)

        # –ü–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        annotation_files = [f for f in os.listdir(ANNOTATION_DIR) if f.endswith('.json')]
        if not annotation_files:
            raise RuntimeError("‚ùå generate_annotations.py –Ω–µ —Å–æ–∑–¥–∞–ª –Ω–∏ –æ–¥–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ.")

    except subprocess.CalledProcessError as e:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ generate_annotations.py:")
        print(e.stderr)
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤—ã—à–µ.")
else:
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(annotation_files)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {ANNOTATION_DIR}.")

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
CLASSES = {
    'background': 0,
    'text': 1,
    'title': 2,
    'author': 3,
    'figure': 4,
    'table': 5,
    'bibliography': 6
}

LABEL_MAP = {
    "Figure": "figure",
    "Table": "table",
    "Text": "text",
    "Title": "title",
    "Author": "author",
    "Bibliography": "bibliography"
}

CLASS_NAMES = list(CLASSES.keys())

# --- –î–∞—Ç–∞—Å–µ—Ç ---
class PDFSegmentationDataset(Dataset):
    def __init__(self, image_root, annotation_dir, transform=None):
        self.image_root = image_root
        self.annotation_dir = annotation_dir
        self.transform = transform

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        self.image_paths = []
        self.annotation_paths = []

        for root, _, files in os.walk(image_root):
            for file in files:
                if file.endswith(".png"):
                    full_image_path = os.path.join(root, file)
                    self.image_paths.append(full_image_path)

                    # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ø–æ–¥–ø–∞–ø–∫–∏ –∏ –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    subfolder = os.path.basename(root)
                    page_number = file.replace("enhanced_page_", "").replace(".png", "")
                    ann_name = f"{subfolder}_enhanced_page_{page_number}.json"

                    self.annotation_paths.append(os.path.join(annotation_dir, ann_name))

        if len(self.image_paths) != len(self.annotation_paths):
            raise RuntimeError("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.")

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        ann_path = self.annotation_paths[idx]

        if not os.path.exists(ann_path):
            raise FileNotFoundError(f"Annotation file not found: {ann_path}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(img_path).convert('RGB')

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        with open(ann_path, 'r') as f:
            annotation = json.load(f)

        mask = np.zeros((image.height, image.width), dtype=np.uint8)

        # –ü–∞—Ä—Å–∏–Ω–≥ Label Studio JSON
        for pred in annotation.get("predictions", []):
            for res in pred.get("result", []):
                value = res.get("value", {})
                if "rectanglelabels" not in value:
                    continue
                label = value["rectanglelabels"][0]
                x = value["x"] / 100
                y = value["y"] / 100
                width = value["width"] / 100
                height = value["height"] / 100

                # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
                label = LABEL_MAP.get(label, label)
                if label not in CLASSES:
                    continue

                class_id = CLASSES[label]

                x1 = int(x * image.width)
                y1 = int(y * image.height)
                x2 = int((x + width) * image.width)
                y2 = int((y + height) * image.height)

                cv2.rectangle(mask, (x1, y1), (x2, y2), class_id, -1)

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        if self.transform:
            image = self.transform(image)

        # –†–µ—Å–∞–π–∑ –º–∞—Å–∫–∏
        mask_resized = cv2.resize(mask, (512, 512), interpolation=cv2.INTER_NEAREST)
        mask_tensor = torch.tensor(mask_resized, dtype=torch.long)

        return image, mask_tensor


# --- –ú–µ—Ç—Ä–∏–∫–∏ ---
def compute_metrics(preds, targets, num_classes):
    preds = preds.view(-1)
    targets = targets.view(-1)

    ious = []
    accuracies = []

    for cls in range(num_classes):
        pred_mask = (preds == cls)
        target_mask = (targets == cls)

        intersection = (pred_mask & target_mask).sum().float()
        union = (pred_mask | target_mask).sum().float()

        iou = intersection / (union + 1e-8) if union > 0 else 1.0
        acc = (pred_mask == target_mask).sum().float() / len(targets)

        ious.append(iou.item())
        accuracies.append(acc.item())

    mean_iou = np.mean(ious)
    mean_acc = np.mean(accuracies)

    return mean_iou, mean_acc, ious, accuracies


# --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ---
def visualize_prediction(model, dataset, device, idx=0):
    model.eval()
    image, mask = dataset[idx]
    image_np = image.permute(1, 2, 0).cpu().numpy()
    image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255
    image_np = image_np.astype(np.uint8)

    with torch.no_grad():
        output = model(image.unsqueeze(0).to(device))
        pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()

    print("Unique values in prediction:", np.unique(pred))

    plt.figure(figsize=(12, 6))

    plt.subplot(1, 3, 1)
    plt.title("Image")
    plt.imshow(image_np)
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.title("True Mask")
    plt.imshow(mask.cpu().numpy(), cmap='jet', vmin=0, vmax=len(CLASSES)-1)
    plt.colorbar()
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.title("Predicted Mask")
    plt.imshow(pred, cmap='jet', vmin=0, vmax=len(CLASSES)-1)
    plt.colorbar()
    plt.axis("off")

    plt.tight_layout()
    plt.savefig("segmentation_result.png")
    plt.show()


# --- –û–±—É—á–µ–Ω–∏–µ ---
def train():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((512, 512)),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    dataset = PDFSegmentationDataset('enhanced_images', 'annotations', transform=transform)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

    model = UNet(n_classes=len(CLASSES))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()

    loss_history = []
    best_val_iou = 0.0
    early_stop_counter = 0
    patience = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π

    start_time = time.time()

    # === –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø ===
    for epoch in range(100):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∏—Å–ª–æ —ç–ø–æ—Ö –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}")
        for images, masks in progress_bar:
            images, masks = images.to(device), masks.to(device)

            outputs = model(images)
            loss = criterion(outputs, masks)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(train_loader)
        loss_history.append(avg_loss)
        print(f"\nEpoch [{epoch + 1}/100], Train Loss: {avg_loss:.4f}")

        # === –í–ê–õ–ò–î–ê–¶–ò–Ø ===
        model.eval()
        total_val_iou = 0
        total_val_acc = 0
        total_samples = 0

        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.to(device)
                outputs = model(images)
                preds = torch.argmax(outputs, dim=1)

                iou, acc, _, _ = compute_metrics(preds, masks, len(CLASSES))
                total_val_iou += iou * images.size(0)
                total_val_acc += acc * images.size(0)
                total_samples += images.size(0)

        avg_val_iou = total_val_iou / total_samples
        avg_val_acc = total_val_acc / total_samples
        print(f"Validation IoU: {avg_val_iou:.4f}, Accuracy: {avg_val_acc:.4f}")

        # === EARLY STOPPING ===
        if avg_val_iou > best_val_iou:
            best_val_iou = avg_val_iou
            early_stop_counter = 0
            torch.save(model.state_dict(), "unet_publaynet_best.pth")
            print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.")
        else:
            early_stop_counter += 1
            print(f"‚ö†Ô∏è –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ IoU —É–∂–µ {early_stop_counter} —ç–ø–æ—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if early_stop_counter >= patience:
            print(f"\nüõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞! –õ—É—á—à–∏–π Validation IoU: {best_val_iou:.4f}")
            break

    end_time = time.time()
    total_time = end_time - start_time
    print(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É: {total_time / (epoch + 1):.2f} —Å–µ–∫—É–Ω–¥")

    # === –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò ===
    torch.save(model.state_dict(), "unet_publaynet_final.pth")
    print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    # === –ì–†–ê–§–ò–ö –ü–û–¢–ï–†–¨ ===
    plt.plot(loss_history, label="Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("–ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–∞–º –¥–ª—è U-Net")
    plt.legend()
    plt.grid(True)
    plt.savefig("loss_curve.png")
    plt.show()

    # === –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ===
    visualize_prediction(model, dataset, device)


if __name__ == "__main__":
    train()