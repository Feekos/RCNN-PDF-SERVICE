import os
import cv2
import numpy as np
import fitz  # PyMuPDF
from PIL import Image
import torch
import torch.nn as nn
from unet_model import UNet
from torchvision import transforms
from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3ForTokenClassification
import pytesseract
import networkx as nx
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier
import pandas as pd
import re
import json

# === –ü–£–¢–ò –ò –§–ê–ô–õ–´ ===
PDF_PATH = "sbornik_1.pdf"
OUTPUT_IMAGES_DIR = "images"
ENHANCED_IMAGES_DIR = "enhanced_images"
SEGMENTED_MASKS_DIR = "segmented_masks"
JSON_OUTPUT = "structured_output.json"

os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)
os.makedirs(ENHANCED_IMAGES_DIR, exist_ok=True)
os.makedirs(SEGMENTED_MASKS_DIR, exist_ok=True)

# === –ö–õ–ê–°–°–´ –î–õ–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò ===
CLASSES = {
    0: 'background',
    1: 'text',
    2: 'title',
    3: 'author',
    4: 'table',
    5: 'figure',
    6: 'bibliography'
}

# === –≠–¢–ê–ü 1: –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø PDF –í –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ===
def pdf_to_images(pdf_path, output_folder):
    doc = fitz.open(pdf_path)
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        pix = page.get_pixmap(dpi=200)
        image_path = os.path.join(output_folder, f"page_{page_num}.png")
        pix.save(image_path)
    doc.close()
    print("‚úÖ PDF –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

# === –≠–¢–ê–ü 2: –£–õ–£–ß–®–ï–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô ===
def enhance_image_quality(image_path, enhanced_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    denoised = cv2.fastNlMeansDenoising(binary, None, h=10)
    cv2.imwrite(enhanced_path, denoised)
    print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_path} —É–ª—É—á—à–µ–Ω–æ.")

# === –≠–¢–ê–ü 3: –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø –° U-NET ===
def load_unet_model(model_path="unet_publaynet.pth", n_classes=7):
    model = UNet(n_classes=n_classes)
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))
    model.eval()
    return model

def segment_page_with_unet(image_path, model, output_mask_path):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    image = Image.open(image_path).convert('RGB').resize((512, 512))
    input_tensor = transform(image).unsqueeze(0)

    with torch.no_grad():
        output = model(input_tensor)
        mask = torch.argmax(output, dim=1).squeeze().numpy()

    cv2.imwrite(output_mask_path, (mask * 255 / mask.max()).astype(np.uint8))
    print(f"‚úÖ –ú–∞—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_mask_path}")
    return mask

# === –≠–¢–ê–ü 4: –ê–ù–ê–õ–ò–ó LAYOUT –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú LayoutLMv3 ===
def detect_layout_elements(image_path):
    feature_extractor = LayoutLMv3FeatureExtractor.from_pretrained("microsoft/layoutlmv3-base")
    model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base")

    image = Image.open(image_path).convert("RGB")
    encoding = feature_extractor(image, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**encoding)
        predictions = torch.argmax(outputs.logits, dim=-1)

    tokens = feature_extractor.tokenizer.convert_ids_to_tokens(encoding["input_ids"][0])
    boxes = encoding["bbox"][0]
    labels = predictions[0]

    elements = []
    for token, box, label_id in zip(tokens, boxes, labels):
        if token.startswith("##") or token in ["[CLS]", "[SEP]"]:
            continue
        label = model.config.id2label[label_id.item()]
        elements.append({
            "token": token,
            "box": box.tolist(),
            "label": label
        })

    print("‚úÖ Layout –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é LayoutLMv3.")
    return elements

# === –≠–¢–ê–ü 5: –ì–†–ê–§–û–í–ê–Ø –ú–û–î–ï–õ–¨ ===
def build_graph(elements):
    G = nx.Graph()
    for i, elem in enumerate(elements):
        G.add_node(i, label=elem['label'], bbox=elem['box'])

    for i in range(len(elements)):
        for j in range(i+1, len(elements)):
            if abs(elements[i]['box'][3] - elements[j]['box'][1]) < 50:
                G.add_edge(i, j)

    print("‚úÖ –ì—Ä–∞—Ñ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
    return G

# === –≠–¢–ê–ü 6: OCR –ü–û –†–ï–ì–ò–û–ù–ê–ú ===
def ocr_by_class(image_array, mask_array):
    image = cv2.cvtColor(np.array(image_array), cv2.COLOR_RGB2BGR)
    result = {}

    class_ids = {
        2: "title",
        3: "author",
        6: "references"
    }

    for cls_id, name in class_ids.items():
        coords = cv2.findNonZero((mask_array == cls_id).astype(np.uint8))
        if coords is None or len(coords) == 0:
            continue
        x, y, w, h = cv2.boundingRect(coords)
        roi = image[y:y+h, x:x+w]
        result[name] = pytesseract.image_to_string(Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))).strip()

    return result

# === –≠–¢–ê–ü 7: –ö–û–ù–¢–ï–ö–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó –ß–ï–†–ï–ó BERT ===
def bert_context_analysis(texts):
    from transformers import BertTokenizer, BertModel
    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
    model = BertModel.from_pretrained('bert-base-multilingual-cased')
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
    return embeddings

# === –≠–¢–ê–ü 8: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –≠–õ–ï–ú–ï–ù–¢–û–í ===
def classify_elements(embeddings):
    X_train = [[100, 50], [200, 150], [50, 80]]
    y_train = ["header", "table", "image"]

    mlp = MLPClassifier(hidden_layer_sizes=(64,))
    mlp.fit(X_train, y_train)
    lgbm = LGBMClassifier()
    lgbm.fit(X_train, y_train)

    predicted_type_mlp = mlp.predict([[120]][[60]])[0]
    predicted_type_lgbm = lgbm.predict([[180]][[140]])[0]

    return predicted_type_mlp, predicted_type_lgbm

# === –≠–¢–ê–ü 9: –≠–ö–°–ü–û–†–¢ –í –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ï –§–û–†–ú–ê–¢–´ ===
def export_data(all_results):
    with open(JSON_OUTPUT, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)
    print(f"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {JSON_OUTPUT}")

# === –ó–ê–ü–£–°–ö ===
if __name__ == "__main__":
    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet_model = load_unet_model().to(device)

    # –®–∞–≥ 1: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    pdf_to_images(PDF_PATH, OUTPUT_IMAGES_DIR)

    # –®–∞–≥ 2: –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    for filename in os.listdir(OUTPUT_IMAGES_DIR):
        if filename.endswith(".png"):
            input_path = os.path.join(OUTPUT_IMAGES_DIR, filename)
            enhanced_path = os.path.join(ENHANCED_IMAGES_DIR, "enhanced_" + filename)
            enhance_image_quality(input_path, enhanced_path)

    all_results = []

    # –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    for filename in os.listdir(ENHANCED_IMAGES_DIR):
        if filename.startswith("enhanced_page_") and filename.endswith(".png"):
            page_num = int(re.search(r'_(\d+)\.png', filename).group(1))
            enhanced_image_path = os.path.join(ENHANCED_IMAGES_DIR, filename)
            mask_path = os.path.join(SEGMENTED_MASKS_DIR, f"mask_page_{page_num}.png")

            # –®–∞–≥ 4: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º U-Net
            mask = segment_page_with_unet(enhanced_image_path, unet_model, mask_path)
            image = Image.open(enhanced_image_path)

            # –®–∞–≥ 5: OCR –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
            segmented_data = ocr_by_class(image, mask)

            # –®–∞–≥ 6: –ê–Ω–∞–ª–∏–∑ layout
            try:
                layout_elements = detect_layout_elements(enhanced_image_path)
            except Exception as e:
                layout_elements = []

            # –®–∞–≥ 7: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            try:
                graph = build_graph(layout_elements)
                graph_data = nx.node_link_data(graph)
            except:
                graph_data = {"nodes": [], "links": []}

            # –®–∞–≥ 8: –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title = segmented_data.get("title", "")
            authors = segmented_data.get("author", "").splitlines()
            references = segmented_data.get("references", "")

            all_results.append({
                "page": page_num,
                "metadata": {
                    "title": title,
                    "authors": authors,
                    "references": references
                },
                "layout_elements": layout_elements,
                "graph": graph_data
            })

    # –®–∞–≥ 9: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ JSON
    export_data(all_results)

    print("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")