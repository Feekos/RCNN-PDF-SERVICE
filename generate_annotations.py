import os
import fitz
import torch
from PIL import Image
import cv2
import numpy as np
import json
from unet_model import UNet
from torchvision import transforms
from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3ForTokenClassification

# === –ü—É—Ç–∏ ===
PDF_PATH = "sbornik_1.pdf"
IMAGES_DIR = "enhanced_images"
ANNOTATIONS_DIR = "annotations"
MODEL_PATH = "unet_publaynet.pth"

os.makedirs(IMAGES_DIR, exist_ok=True)
os.makedirs(ANNOTATIONS_DIR, exist_ok=True)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ ===
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CLASSES = {
    0: 'background',
    1: 'text',
    2: 'title',
    3: 'author',
    4: 'figure',
    5: 'table',
    6: 'bibliography'
}

# === –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((512, 512)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# === –ó–∞–≥—Ä—É–∑–∫–∞ U-Net ===
def load_unet_model(model_path, n_classes=7):
    model = UNet(n_classes=n_classes)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    return model

# === –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã ===
def segment_page_with_unet(image_path, model):
    image = Image.open(image_path).convert("RGB").resize((512, 512))
    input_tensor = transform(image).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        output = model(input_tensor)
        mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()

    print(f"‚úÖ –ú–∞—Å–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞ –¥–ª—è {image_path}")
    return image, mask

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Å–∫–∏ ===
def generate_annotation_from_mask(mask, image_size=(512, 512)):
    annotations = []

    for cls_id, label in CLASSES.items():
        if cls_id == 0:
            continue  # –ü—Ä–æ–ø—É—Å–∫ —Ñ–æ–Ω–∞

        coords = cv2.findNonZero((mask == cls_id).astype(np.uint8))
        if coords is None or len(coords) == 0:
            continue

        x, y, w, h = cv2.boundingRect(coords)
        annotations.append({
            "original_width": image_size[0],
            "original_height": image_size[1],
            "image_rotation": 0,
            "value": {
                "x": int(x / image_size[0] * 100),
                "y": int(y / image_size[1] * 100),
                "width": int(w / image_size[0] * 100),
                "height": int(h / image_size[1] * 100),
                "rotation": 0
            },
            "from_name": "label",
            "to_name": "image",
            "type": "RectangleLabels",
            "value": {"rectanglelabels": [label]}
        })

    return annotations

# === –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ layout —á–µ—Ä–µ–∑ LayoutLMv3 (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ===
def detect_layout_elements(image_path):
    feature_extractor = LayoutLMv3FeatureExtractor.from_pretrained("microsoft/layoutlmv3-base")
    model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base").to(DEVICE)

    image = Image.open(image_path).convert("RGB")
    encoding = feature_extractor(image, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model(**encoding)
        predictions = torch.argmax(outputs.logits, dim=-1)

    tokens = feature_extractor.tokenizer.convert_ids_to_tokens(encoding["input_ids"][0])
    boxes = encoding["bbox"][0]
    labels = predictions[0]

    elements = []
    for token, box, label_id in zip(tokens, boxes, labels):
        if token.startswith("##") or token in ["[CLS]", "[SEP]"]:
            continue
        label = model.config.id2label[label_id.item()]
        elements.append({
            "token": token,
            "box": box.tolist(),
            "label": label
        })

    return elements

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Label Studio ===
def save_label_studio_annotation(image_name, annotation_data):
    result = [{
        "id": "result1",
        "from_name": "label",
        "to_name": "image",
        "type": "RectangleLabels",
        "value": data
    } for data in annotation_data]

    annotation = {
        "data": {"image": f"/data/local-files/?d={image_name}"},
        "predictions": [{
            "model_version": "layout-unet-v1",
            "result": result
        }]
    }

    filename = os.path.splitext(os.path.basename(image_name))[0] + ".json"
    with open(os.path.join(ANNOTATIONS_DIR, filename), "w", encoding="utf-8") as f:
        json.dump(annotation, f, indent=2, ensure_ascii=False)

# === –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π ===
if __name__ == "__main__":
    print("üöÄ –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")

    # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    unet_model = load_unet_model(MODEL_PATH)

    # –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    for filename in os.listdir(IMAGES_DIR):
        if filename.endswith(".png"):
            image_path = os.path.join(IMAGES_DIR, filename)
            image, mask = segment_page_with_unet(image_path, unet_model)

            # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            annotation_data = generate_annotation_from_mask(mask)

            # –®–∞–≥ 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Label Studio
            save_label_studio_annotation(filename, annotation_data)

    print(f"‚úÖ –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ {ANNOTATIONS_DIR}")